{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece protobuf flash-attn bitsandbytes autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ddc8cda7894badb95ddaeec2a9bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99be7d6e09904bc2a874843afb5d15ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3c8986b6984b2a9cd250968b0f6fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3c692af8e6439b854cfe7b0b1c015a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c6653638994958abc255aed457258f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from configuration_kraken import KrakenConfig\n",
    "from modeling_kraken import KrakenForCausalLM\n",
    "\n",
    "AutoConfig.register(\"kraken\", KrakenConfig)\n",
    "AutoModelForCausalLM.register(KrakenConfig, KrakenForCausalLM)\n",
    "\n",
    "# Load the model and config\n",
    "config = AutoConfig.from_pretrained(\"./kraken_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./kraken_model\", config=config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Du bist ein hilfreicher und freundlicher deutscher KI-Assistent.'}, {'role': 'user', 'content': 'Erzähle mir eine Gute Nacht Geschichte in 2 Sätzen.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing expert5 ..\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Du bist ein hilfreicher und freundlicher deutscher KI-Assistent.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Erzähle mir eine Gute Nacht Geschichte in 2 Sätzen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In einem kleinen Dorf in der Schwarzwaldregion lebte ein kleiner Junge namens Tim, der so sehr an den Sternen hing, dass er eines Tages in einem kleinen, schneebedeckten Haus in den Wolken wohnen wollte. Als er eines Abends vor dem Schlafengehen hinausschaute, sah er, dass die Sterne sich zu einem leuchtenden Bogen zusammenfanden, um ihn zu grüßen, bevor er in sein Bett fiel und in tiefen Schlaf sank.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# german\n",
    "#english expert 1(label 0)\n",
    "#fc expert 2(label 1)\n",
    "#python expert 3(label 2)\n",
    "#sql expert 4(label 3)\n",
    "#german expert 5(label 4)\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'Du bist ein hilfreicher und freundlicher deutscher KI-Assistent.'},\n",
    "    {'role': 'user', 'content': 'Erzähle mir eine Gute Nacht Geschichte in 2 Sätzen.'}\n",
    "    ]\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "#test=tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#print(input_text)\n",
    "output_ids = model.generate(input_ids ,temperature=0.4, do_sample=True, top_p=0.9,top_k=20, max_length=250)\n",
    "print(model.expert_tokenizer(text=input_text).decode(output_ids[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpul AI assistant.'}, {'role': 'user', 'content': \"Generate a SQL query to answer this question: What is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\nDDL statements:\\nCREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\"}]\n",
      "Choosing expert4 ..\n",
      "system\n",
      "\n",
      "You are a helpul AI assistant.user\n",
      "\n",
      "Generate a SQL query to answer this question: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "\n",
      "DDL statements:\n",
      "CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');assistant\n",
      "\n",
      "SELECT s.name AS salesperson_name, SUM(t.volume) AS total_volume_sold FROM timber_sales t JOIN salesperson s ON t.salesperson_id = s.salesperson_id GROUP BY s.name ORDER BY total_volume_sold DESC NULLS LAST;\n"
     ]
    }
   ],
   "source": [
    "#sql\n",
    "\n",
    "#english expert 1(label 0)\n",
    "#fc expert 2(label 1)\n",
    "#python expert 3(label 2)\n",
    "#sql expert 4(label 3)\n",
    "#german expert 5(label 4)\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpul AI assistant.'},\n",
    "    {'role': 'user', 'content': \"\"\"Generate a SQL query to answer this question: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
    "\n",
    "DDL statements:\n",
    "CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\"\"\"}\n",
    "    ]\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "output_ids = model.generate(input_ids ,temperature=0.6, do_sample=True, top_p=0.9,top_k=20, max_length=500)\n",
    "print(model.expert_tokenizer(text=input_text).decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Create a python function to calculate the sum of a sequence of integers.\n",
      "    [1, 2, 3, 4, 5]<|im_end|>\n",
      "\n",
      "[{'role': 'system', 'content': '<|im_start|>system'}, {'role': 'user', 'content': 'Create a python function to calculate the sum of a sequence of integers.\\n    [1, 2, 3, 4, 5]'}]\n",
      "Choosing expert3 ..\n",
      "You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n",
      "\n",
      "@@ Instruction\n",
      "Create a python function to calculate the sum of a sequence of integers.\n",
      "    [1, 2, 3, 4, 5]\n",
      "\n",
      "@@ Response\n",
      "Here is a simple Python function that calculates the sum of a sequence of integers:\n",
      "\n",
      "```python\n",
      "def sum_sequence(sequence):\n",
      "    return sum(sequence)\n",
      "\n",
      "print(sum_sequence([1, 2, 3, 4, 5]))\n",
      "```\n",
      "\n",
      "In this function, we use the built-in `sum` function in Python, which adds up all the elements in a list or other iterable. We pass our sequence to this function and it returns the sum of the elements in the sequence.\n"
     ]
    }
   ],
   "source": [
    "#python \n",
    "\n",
    "#english expert 1(label 0)\n",
    "#fc expert 2(label 1)\n",
    "#python expert 3(label 2)\n",
    "#sql expert 4(label 3)\n",
    "#german expert 5(label 4)\n",
    "messages = [\n",
    "    {'role': 'system', 'content': ''},\n",
    "    {'role': 'user', 'content': \"\"\"Create a python function to calculate the sum of a sequence of integers.\n",
    "    [1, 2, 3, 4, 5]\"\"\"}\n",
    "    ]\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "output_ids = model.generate(input_ids ,temperature=0.6, do_sample=True, top_p=0.9,top_k=20, max_length=250)\n",
    "print(model.expert_tokenizer(text=input_text).decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistant with access to the following functions. Use them if required -\\n    { \"name\": \"calculate_area\", \"description\": \"Calculate the area of a shape\", \"parameters\": { \"type\": \"object\", \"properties\": { \"shape\": { \"type\": \"string\", \"description\": \"The type of shape (circle, rectangle, triangle, etc.)\" }, \"dimensions\": { \"type\": \"object\", \"properties\": { \"length\": { \"type\": \"number\", \"description\": \"The length of the shape\" }, \"width\": { \"type\": \"number\", \"description\": \"The width of the shape\" }, \"base\": { \"type\": \"number\", \"description\": \"The base of the shape\" }, \"height\": { \"type\": \"number\", \"description\": \"The height of the shape\" } } } }, \"required\": [ \"shape\" ] } }'}, {'role': 'user', 'content': 'I need to calculate the area of a rectangle. The length is 5 and the width is 3.'}]\n",
      "Choosing expert2 ..\n",
      "You are a helpful assistant with access to the following functions. Use them if required -\n",
      "    { \"name\": \"calculate_area\", \"description\": \"Calculate the area of a shape\", \"parameters\": { \"type\": \"object\", \"properties\": { \"shape\": { \"type\": \"string\", \"description\": \"The type of shape (circle, rectangle, triangle, etc.)\" }, \"dimensions\": { \"type\": \"object\", \"properties\": { \"length\": { \"type\": \"number\", \"description\": \"The length of the shape\" }, \"width\": { \"type\": \"number\", \"description\": \"The width of the shape\" }, \"base\": { \"type\": \"number\", \"description\": \"The base of the shape\" }, \"height\": { \"type\": \"number\", \"description\": \"The height of the shape\" } } } }, \"required\": [ \"shape\" ] } }### Instruction:\n",
      "I need to calculate the area of a rectangle. The length is 5 and the width is 3.\n",
      "### Response:\n",
      "<<function>>calculate_area(shape='rectangle', dimensions={'length': 5, 'width': 3})\n"
     ]
    }
   ],
   "source": [
    "#function calling \n",
    "messages = [\n",
    "    {'role': 'system', 'content': \"\"\"You are a helpful assistant with access to the following functions. Use them if required -\n",
    "    { \"name\": \"calculate_area\", \"description\": \"Calculate the area of a shape\", \"parameters\": { \"type\": \"object\", \"properties\": { \"shape\": { \"type\": \"string\", \"description\": \"The type of shape (circle, rectangle, triangle, etc.)\" }, \"dimensions\": { \"type\": \"object\", \"properties\": { \"length\": { \"type\": \"number\", \"description\": \"The length of the shape\" }, \"width\": { \"type\": \"number\", \"description\": \"The width of the shape\" }, \"base\": { \"type\": \"number\", \"description\": \"The base of the shape\" }, \"height\": { \"type\": \"number\", \"description\": \"The height of the shape\" } } } }, \"required\": [ \"shape\" ] } }\"\"\"},\n",
    "    {'role': 'user', 'content': \"\"\"I need to calculate the area of a rectangle. The length is 5 and the width is 3.\"\"\"}\n",
    "    ]\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "output_ids = model.generate(input_ids ,temperature=0.6, do_sample=True, top_p=0.9,top_k=20, max_length=250)\n",
    "print(model.expert_tokenizer(text=input_text).decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful AI Assistant'}, {'role': 'user', 'content': 'Find the mass percentage of Ba in BaO'}]\n",
      "Choosing expert1 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      "You are a helpful AI Assistant \n",
      "<|im_start|> user\n",
      "Find the mass percentage of Ba in BaO \n",
      "<|im_start|> assistant\n",
      "To find the mass percentage of Ba in BaO, we need to know the molar masses of barium (Ba) and oxygen (O), and then use the formula:\n",
      "\n",
      "Mass percentage of Ba = (mass of Ba in BaO / molar mass of BaO) x 100%\n",
      "\n",
      "The molar mass of barium (Ba) is approximately 137.33 g/mol, and the molar mass of oxygen (O) is approximately 16.00 g/mol.\n",
      "\n",
      "The molar mass of barium oxide (BaO) is the sum of the molar masses of Ba and O:\n",
      "Molar mass of BaO = molar mass of Ba + molar mass of O\n",
      "Molar mass of BaO = 137.33 g/mol + 16.00 g/mol\n",
      "Molar mass of BaO = 153.33 g/mol\n",
      "\n",
      "Now, we can calculate the mass percentage of Ba in BaO:\n"
     ]
    }
   ],
   "source": [
    "#english \n",
    "messages = [\n",
    "    {'role': 'system', 'content': \"\"\"You are a helpful AI Assistant\"\"\"},\n",
    "    {'role': 'user', 'content': \"\"\"Find the mass percentage of Ba in BaO\"\"\"}\n",
    "    ]\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "output_ids = model.generate(input_ids ,temperature=0.6, do_sample=True, top_p=0.9,top_k=20, max_length=250)\n",
    "print(model.expert_tokenizer(text=input_text).decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
